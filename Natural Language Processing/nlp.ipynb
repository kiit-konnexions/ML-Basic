{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/workspaces/ML-Basic/.conda/nltk_data'\n    - '/workspaces/ML-Basic/.conda/share/nltk_data'\n    - '/workspaces/ML-Basic/.conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokenize\u001b[39;00m \u001b[39mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m      2\u001b[0m cor \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMy name is Sandeep of CSE-10, currently living in BBSR. Dr. John has done ph.D at IIT Madras located in Tamil Nadu on 24/01/2023. And yay! we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre going to meet soon\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sent \u001b[39m=\u001b[39m sent_tokenize(cor)\n\u001b[1;32m      4\u001b[0m sent\n",
      "File \u001b[0;32m/workspaces/ML-Basic/.conda/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/workspaces/ML-Basic/.conda/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m/workspaces/ML-Basic/.conda/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m/workspaces/ML-Basic/.conda/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/codespace/nltk_data'\n    - '/workspaces/ML-Basic/.conda/nltk_data'\n    - '/workspaces/ML-Basic/.conda/share/nltk_data'\n    - '/workspaces/ML-Basic/.conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "cor = \"My name is Sandeep of CSE-10, currently living in BBSR. Dr. John has done ph.D at IIT Madras located in Tamil Nadu on 24/01/2023. And yay! we're going to meet soon\"\n",
    "sent = sent_tokenize(cor)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Sandeep of CSE-10, currently living in BBSR.\n",
      "Dr. John has done ph.D at IIT Madras located in Tamil Nadu on 24/01/2023.\n",
      "And yay!\n",
      "we're going to meet soon\n"
     ]
    }
   ],
   "source": [
    "for i in sent:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'John',\n",
       " 'has',\n",
       " 'done',\n",
       " 'ph.D',\n",
       " 'at',\n",
       " 'IIT',\n",
       " 'Madras',\n",
       " 'located',\n",
       " 'in',\n",
       " 'Tamil',\n",
       " 'Nadu',\n",
       " 'on',\n",
       " '24/01/2023',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token = word_tokenize(sent[1])\n",
    "token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.\n",
      "john\n",
      "ha\n",
      "done\n",
      "ph.d\n",
      "at\n",
      "iit\n",
      "madra\n",
      "locat\n",
      "in\n",
      "tamil\n",
      "nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in token:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.\n",
      "john\n",
      "has\n",
      "don\n",
      "ph.d\n",
      "at\n",
      "iit\n",
      "madra\n",
      "loc\n",
      "in\n",
      "tamil\n",
      "nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "for i in token:\n",
    "    print(ls.stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.\n",
      "john\n",
      "has\n",
      "done\n",
      "ph.d\n",
      "at\n",
      "iit\n",
      "madra\n",
      "locat\n",
      "in\n",
      "tamil\n",
      "nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer('english')\n",
    "for i in token:\n",
    "    print(ss.stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization using WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "John\n",
      "ha\n",
      "done\n",
      "ph.D\n",
      "at\n",
      "IIT\n",
      "Madras\n",
      "located\n",
      "in\n",
      "Tamil\n",
      "Nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "for i in token:\n",
    "    print(wl.lemmatize(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dr.', 'NNP'),\n",
       " ('John', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('done', 'VBN'),\n",
       " ('ph.D', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('IIT', 'NNP'),\n",
       " ('Madras', 'NNP'),\n",
       " ('located', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Tamil', 'NNP'),\n",
       " ('Nadu', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('24/01/2023', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tag(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy for lemmatisation and pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my PRON poss\n",
      "name name NOUN nsubj\n",
      "is be AUX ROOT\n",
      "Sandeep Sandeep PROPN attr\n",
      "of of ADP prep\n",
      "CSE-10 CSE-10 PROPN pobj\n",
      ", , PUNCT punct\n",
      "currently currently ADV advmod\n",
      "living live VERB advcl\n",
      "in in ADP prep\n",
      "BBSR BBSR PROPN pobj\n",
      ". . PUNCT punct\n",
      "Dr. Dr. PROPN compound\n",
      "John John PROPN nsubj\n",
      "has have AUX aux\n",
      "done do VERB ROOT\n",
      "ph ph NOUN compound\n",
      ". . PUNCT compound\n",
      "D D PROPN dobj\n",
      "at at ADP prep\n",
      "IIT IIT PROPN compound\n",
      "Madras Madras PROPN pobj\n",
      "located locate VERB acl\n",
      "in in ADP prep\n",
      "Tamil Tamil PROPN compound\n",
      "Nadu Nadu PROPN pobj\n",
      "on on ADP prep\n",
      "24/01/2023 24/01/2023 NUM pobj\n",
      ". . PUNCT punct\n",
      "And and CCONJ cc\n",
      "yay yay INTJ ROOT\n",
      "! ! PUNCT punct\n",
      "we we PRON nsubj\n",
      "'re be AUX aux\n",
      "going go VERB ROOT\n",
      "to to PART aux\n",
      "meet meet VERB xcomp\n",
      "soon soon ADV advmod\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(cor)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandeep GPE\n",
      "John PERSON\n",
      "IIT Madras ORG\n",
      "Tamil Nadu PERSON\n",
      "24/01/2023 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'my'), ('my', 'name'), ('name', 'is'), ('is', 'Sandeep'), ('Sandeep', 'and'), ('and', 'I'), ('I', 'am'), ('am', 'a'), ('a', 'student'), ('student', 'from'), ('from', 'BBSR'), ('BBSR', '.')]\n",
      "[('Hello', 'my', 'name'), ('my', 'name', 'is'), ('name', 'is', 'Sandeep'), ('is', 'Sandeep', 'and'), ('Sandeep', 'and', 'I'), ('and', 'I', 'am'), ('I', 'am', 'a'), ('am', 'a', 'student'), ('a', 'student', 'from'), ('student', 'from', 'BBSR'), ('from', 'BBSR', '.')]\n",
      "[('Hello', 'my', 'name', 'is'), ('my', 'name', 'is', 'Sandeep'), ('name', 'is', 'Sandeep', 'and'), ('is', 'Sandeep', 'and', 'I'), ('Sandeep', 'and', 'I', 'am'), ('and', 'I', 'am', 'a'), ('I', 'am', 'a', 'student'), ('am', 'a', 'student', 'from'), ('a', 'student', 'from', 'BBSR'), ('student', 'from', 'BBSR', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "token = nltk.word_tokenize(\"Hello my name is Sandeep and I am a student from BBSR.\")\n",
    "\n",
    "print(list(bigrams(token)))\n",
    "print(list(trigrams(token)))\n",
    "print(list(ngrams(token, 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
